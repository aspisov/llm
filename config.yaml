# Training Configuration for CS336 Basics Transformer

# Model architecture
model:
  vocab_size: 10000
  context_length: 1024
  num_layers: 12
  d_model: 768
  num_heads: 12
  d_ff: 3072  # 768 * 4
  rope_theta: 10000

# Training parameters
training:
  max_lr: 0.001
  min_lr: 0.00001
  warmup_iters: 20
  cosine_cycle_iters: 100
  betas: [0.9, 0.95]
  weight_decay: 0.01
  num_iterations: 100
  batch_size: 2
  val_frequency: 10
  max_l2_norm: 10

# Data paths
data:
  train_path: "data/TinyStories-valid.npz"
  val_path: "data/TinyStories-valid.npz"

# Checkpointing
checkpoints:
  initial_checkpoint: null
  checkpoints_path: "outputs/checkpoints"
  checkpoint_frequency: null

# Hardware
hardware:
  device: "mps"
  dtype: "float32"