# Training Configuration for CS336 Basics Transformer

# Model architecture
model:
  vocab_size: 10000
  context_length: 256
  num_layers: 4
  d_model: 512
  num_heads: 16
  d_ff: 1344
  rope_theta: 10000

# Training parameters
training:
  max_lr: 0.001
  min_lr: 0.00001
  warmup_iters: 30
  cosine_cycle_iters: 3000
  betas: [0.9, 0.95]
  weight_decay: 0.01
  num_iterations: 3001
  batch_size: 4
  val_frequency: 100
  max_l2_norm: 10

# Data paths
data:
  train_path: "data/TinyStories-valid.npz"
  val_path: "data/TinyStories-valid.npz"

# Checkpointing
checkpoints:
  initial_checkpoint: "outputs/checkpoints/final_checkpoint.pt"
  checkpoints_path: "outputs/checkpoints"
  checkpoint_frequency: 250

# Hardware
hardware:
  device: "mps"
  dtype: "float32"